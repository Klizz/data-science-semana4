{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 4\n",
    "Durante las semanas previas aprendieron a usar varias herramientas de Python; desde Python vanilla (sin el uso de paquetes) hasta el uso de los paquetes más improtantes para el análisis numérico en Python (Numpy, Matplotlib y Pandas). Con estas herramientas se puede realizar gran parte del análisis estadístico de datos; sin embargo, esta semana no lidiara con el análisis de datos; de hecho se podría decir que estamos dando un paso atrás en el proceso de Data Science.\n",
    "\n",
    "<img src=\"img/dataprog.png\"/>\n",
    "\n",
    "Nosotros nos vamos a enfocar en el área de Data Gathering; el hecho de que el diagrama no haga énfasis en esta parte del proceso no le resta su importancia ya que si no tenemos buenos datos no podemos tener buenas conclusiones. En esta semana vamos a seguir el proceso de obtención y pre-procesamiento de datos: Primero veremos Data Modeling para ver las formas en las cuales los datos se relacionan y se guardan; después veremos como podemos obtener datos de internet de una forma eficiente y finalmente haremos actividades para poner a prueba los conocimientos de esta semana.\n",
    "\n",
    "# Objetivos\n",
    "- Conocer un poco acerca del proceso de obtención y almacenamiento de datos.\n",
    "- Ser capaces navegar los tags en un sitio de internet.\n",
    "- Aprenda a utilizar la librería requests y beautifulsoup4 para obtener información de estos sitios.\n",
    "- Conozca lo que es una API y sea capaz de utilizar API's básicas.\n",
    "- Saber como crear un bot (araña) para automatizar de mejor forma el scrappeo de la info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Como podemos obtener datos?\n",
    "Los datos en general se pueden obtener de muchas formas; puede ser a través de dispositivos electrónicos (sensores) o de una interacción más humana (encuestas telefónicas o en línea), la verdad es que las formas de obtener datos son tan diversas como los datos mismos y estas suelen cambiar sin embargo, dependiendo de la situación en la cual nos encontremos vamos a necesitar diferentes tipos de datos. En el ámbito empresarial nos interesarían datos internos de nuestra empresa como lo son datos de empleados, desempeño de programas, indicadores financieros, días de inventario, entre otros; también nos pueden interesar datos de entes externos como proveedores, almacenistas, transportistas y clientes. Ya que reducimos la variedad de datos ahora podemos recolectarlos y eventualmente almacenarlos; sin embargo, este no es un proceso de caja negra. Este proceso necesita de conocimiento previo de los datos, las entidades involucradas y conocer los esquemas de almacenamiento para escoger al que vaya a ser de más ayuda en nuestro caso. Para esto se usan modelos de datos y vamos a ver las bases suficientes para planear de forma correcta el almacenamiento de datos.\n",
    "\n",
    "## Data modeling\n",
    "Un modelo de datos es una representación (generalmente gráfica) en la cual se pueden observar los contenidos, relaciones y jerarquías de los elementos que conforman una base de datos. Es un esqueleto con las reglas que van a regir nuestra base de datos; por lo tanto, debe de ser rígido pero lo suficientemente robusto para evitar errores; por lo tanto, hay que seguir un proceso cuando vayamos a querer crear una base de datos para asegurarnos de que la base de datos esté orientada a las necesidades de la empresa. El proceso más seguido para la creación de una base de datos es seguir el orden de (Conceptual, Logical y Physical) el cual nos permite organizar nuestros datos casi independientemente del software que utilizemos para guardar los datos. \n",
    "\n",
    "<img src=\"img/Datamod.png\"/>\n",
    "\n",
    "### Modelo Conceptual\n",
    "\n",
    "El modelo conceptual se hace para explicar la relación de las entidades (personas, empresas, almacenes, etc.) a gente fuera del área de programación; es decir, se observa más al comportamiento de las entidades que la información numérica contenida. En el ejemplo anterior estamos viendo una base de datos sencilla en donde tenemos 3 entidades (Estudiantes, Cursos y cursos inscritos). En este diagrama apreciamos que la entidad (cursos inscritos) necesita del estudiante y de la lista global de cursos; esto nos dice que a un estudiante se le asignan ciertos cursos inscritos.\n",
    "\n",
    "La ventaja de usar inicialmente un modelo conceptual es que todavía no pensamos en la información; cuando nuestro enfoque principal son los individuos y no los datos esto nos permite mostrar el sistema de la forma más sencilla y general. Una vez que se tengan todas las entidades entonces ya se pueden trabajar sobre ellas e ir agregando datos lo cual llevará al siguiente modelo.\n",
    "\n",
    "### Modelo Lógico\n",
    "\n",
    "Regresando a la imágen; podemos ver que el modelo lógico ocupa más espacio que el conceptual y con mucha razón. Un modelo lógico todavía no considera que herramienta vamos a usar para guardar los datos; sin embargo, ahora ya nos enfocamos en los datos contenidos en cada entidad y las relaciones entre ellos. Regresando al caso de la universidad; ahora vemos que el estudiante cuenta con varios atributos que lo identifican como su matrícula, nombre y correo. También podemos ver que el curso tiene sus propiedades como ID, nombre, instructor y horario, es por medio de la combinación de ambos datos que podemos tener una inscripción a la cual se le asocia un estudiante y cursos; sin embargo, la inscripción tiene sus propios datos como calificaciones y estatus. Cuando antes solo veiamos una relación simple entre las entidades; ahora podemos ver que elementos conforman esa relación. Este modelo es de particular importancia ya que se introduce el concepto de llave primaria (primary key), llave externa (foreign key) y llave compuesta (composite key); estos conceptos permiten asociar identificadores a cada entidad de nuestra base de datos que son de gran ayuda tanto en el proceso de diseño como en el proceso de extracción de la data.\n",
    "\n",
    "El modelo lógico sigue siendo independiente del sistema de administración de datos; por lo cual por ahora no nos concentramos mucho en como van a estar almacenados ni en el tipo de variable. Una vez que tengamos establecido el modelo lógico ya podemos pasar a la implementación final.\n",
    "\n",
    "### Modelo Físico\n",
    "\n",
    "El modelo físico es lo más similar que vamos a tener a la base de datos; en lugar de solamente tener los elementos de la relación estos son desmenuzados hasta sus componentes fundamentales. Por ejemplo: Si volvemos a ver la casilla de estudiante podemos ver que Address se ha dividido en (Address1, Address2, postal code, city y state) que son partes del elemento Address. No solo se han encontrado las componentes fundamentales; sino también se indica de que tipo va a ser la variable y en algunos casos la longitud máxima que puede tener (esto ya termina afectando el sistema de administración de de datos). Este modelo físico es el que ya se terminaría implementando para establecer las reglas de todos los futuros datos que van a entrar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs NoSQL (Relacional vs Documento)\n",
    "Una vez que hemos pasado por el proceso de crear el modelo físico, es hora de seleccionar un sistema de base de datos. Hay que estar muy seguros de que conozcamos bien las relaciones encontradas en el modelo físico ya que de ello depende el tipo de sistema que vamos a usar. Los dos sistemas dominantes son SQL (Structured Query Language) y NoSQL (Not only SQL); el modelo SQL se basa en el uso de tablas altamente estructuradas y ordenadas que se asemejan bastante a un excel o como vemos los Pandas en Python. Tenemos columnas que representan elementos o datos y renglones que representan observaciones; por lo tanto, todas las observaciones tienen el mismo número de elementos asociados a ellos.\n",
    "\n",
    "<img src=\"img/SQL.png\"/>\n",
    "\n",
    "\n",
    "Un ejemplo de datos que se puede implementar facilmente en SQL es un registro de transacciones; cada transacción tiene un emisor, un receptor, una cantidad y una fecha. Debido a que todos los elementos son muy similares entonces se pueden usar una o más tablas para representar toda la información. Por lo tanto; la ventaja de SQL es que la estructura rígida puede facilitar mucho la accessibilidad de los datos; asimismo, es un sistema que lleva vigente desde los 60's sin ser reemplazado por lo cual la comunidad y el soporte están muy bien fundamentados; sin embargo, también cuenta con sus desventajas. Muchos programas que usan SQL no son open-source y estos sistemas no son tan escalables debido a su funcionamiento. Si uno quisiera incrementar la capacidad de esto se necesitaria aumentar las especificaciones de 1 solo servidor por lo cual hay una limitante tecnológica a esto.\n",
    "\n",
    "Por otra parte; NoSQL renuncia a la estructura por medio de tablas por lo cual también renuncia a varios de los beneficios de SQL; sin embargo, mucha información se encuentra guardad en este formato debido a que es fácil de procesar multiples requests lo que facilita su escalamiento. Los sensores e información proveniente de (Big Data) se maneja en este formato por lo cual su relevancia no puede ser ignorada. NoSQL sigue un formato libre (en este caso nos enfocamos en el modelo de documento) lo cual nos permite almacenar la información dentro de documentos json o en un formato anidado. ¿Que otra ventaja nos da? Para ello hay que ver relaciones lo cual suena extraño ya que esto no es una base de datos relacional. \n",
    "\n",
    "### Relaciones\n",
    "Un elemento puede tener varios tipos de relaciones (many to many), (one to many) o (many to one). Por ejemplo; supongamos que tenemos una base de datos en donde se guarda la información de aplicantes a una empresa. Ahí tenemos valores únicos como nombre, correo y dirección; sin embargo, no todos cuentan con el mismo número de trabajos previos. El campo de trabajos tiene una relación one to many ya que es muy poco probable que diferentes personas tengan la misma experiencia en las mismas empresas; asimismo, varios aplicantes pueden ir a diferentes escuelas pero en general estas se repiten entre personas (many to many). En el caso one to many y many to many es cuando se recomendaría utilizar NoSQL sobre SQL; sin embargo, esto no significa que los datos no pueden ser representados usando SQL solo que su almacenamiento se puede complicar y ser ineficiente comparado con las soluciones NoSQL.\n",
    "\n",
    "### ¿Que nos podemos llevar de esto?\n",
    "Lo que podemos ver es que antes de pensar en conseguir y utilizar información se tiene que saber bajo que esquema esta se va a guardar. Las conexiones, relaciones y forma de almacenamiento de los datos son cuestiones que pueden terminar haciendo más difícil o imposible el uso de esos datos para proyectos interesantes. Por lo tanto; tenemos que desarrollar buenas prácticas para que nuestros datos tengan una estructura que esté de acuerdo a su naturaleza.\n",
    "\n",
    "## Webscrapping\n",
    "La definición de Webscrapping es la obtención y preprocesamiento de datos que se originan de internet; ahora, aunque tengamos acceso a mucha información por medio de preguntas, encuestas y bases de datos locales de la empresa; no podemos negar que la mayor fuente de información del mundo es el internet. Desde Amazon para un análisis del posicionamiento digital de la competencia, hasta la INEGI para realizar un estudio demográfico o obtener indicadores macroeconómicos; en la actualidad, casi todo se puede encontrar de una forma u otra en línea y muchas empresas e individuos están buscando aprovechar esto para hacer proyectos interesantes.\n",
    "\n",
    "### ¿Como se estructuran los elementos de una página web?\n",
    "Aunque la apariencia de una página web puede parecer ordenada al usuario; al entrar y ver el código se puede comenzar a ver que tan complicado puede llegar a ser el scrapping. Hay muchos elementos con diferentes jerarquías; elementos que contienen elementos dentro de otros elementos. Cuando hacemos Webscrapping en realidad estámos recuperando el código html de la página web por lo que es importante que sepamos navegarlo para recuperar la información que nos interesa. Este proceso no es fácil pero utilizando Python este se puede hacer un poco menos complicado.\n",
    "<img src=\"img/html.png\"/>\n",
    "<img src=\"img/dom-tree.png\" />\n",
    "\n",
    "Al contar con una estructura anidada; podemos ver que las páginas web no son relacionales debido a que cuentan con diferentes niveles y jerarquías para sus elementos. Aunque la representación relacional es posible; la no relacional termina siendo la más intuitiva en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La ética en la ciencia de datos\n",
    "Reuerden que sin importar de donde estemos adquiriendo la información, detrás de esos números se encuentran seres humanos como nosotros por lo cual es importante no solo cuidarla sino darle un uso apropiado. Actualmente no hay leyes tan fuertes en cuanto al uso y venta de información; sin embargo, solo por que la ley no lo prohibe explicitamente no significa que podemos hacer lo que queramos con la información. Ya hay casos muy sonados de recollección y venta de información de grandes empresas como Facebook y Google; sin embargo, estos casos pueden terminar sin \n",
    "\n",
    "Consejos de la ética.\n",
    "\n",
    "1. Conoce los derechos legales y humanos, respétalos. \n",
    "2. Nuestra aplicación permite la rendición de cuentas.\n",
    "3. La información puede ser abierta, pero eso no significa que esta sea libre de uso para cualquier aplicación. \n",
    "4. Mantén la privacidad del usuario; no permitas que en alguna parte del proceso se filtre la información personal de este. \n",
    "5. La información siempre tiene un fin, establecelos claramente en el aviso de privacidad.\n",
    "6. Piensa en como estás introduciendo sesgo en tu análisis y como este puede afectar a los demás.\n",
    "7. En resumen; no solo pienses en el bien de la empresa, accionistas o inclusive los clientes. Piensa en las consecuencias que tiene para todos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora a programar!\n",
    "\n",
    "## Requests\n",
    "Cuando queremos acceder a una página de internet nosotros tecleamos la dirección en el navegador; después, nuestro navegador hace una petición al servidor para recibir la información acerca de la página a la que queremos acceder. Un request es una petición para entrar a la página de nuestro interés; ahora que se hace un request a la página se debe de decidir el método que se va a realizar.\n",
    "\n",
    "Lo primero que podemos hacer es obtener la información, esto se hace con el get. Es una solicitud para recuperar el código de la página de internet. Esté método se puede utilizar en python importando el paquete de requests; al correr el método dir sobre el objeto request podemos obtener una lista que nos dice la información y las etiquetas de la página que podemos acceder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get('http://www.amazon.com')\n",
    "#dir(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, recuperamos un objeto muy grande que puede tener mucha información pero esta no se encuentra tan a la vista como lo puede ser en un documento word. Si queremos acceder a esta información tenemos que utilizar la notación de Python de .objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/ <PreparedRequest [GET]> ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "print(page.url, page.request, page.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque si vemos la página, podemos ver que no tenemos los permisos apropiados para scrapear amazon y nos regresa un mensaje automatizado para poder scrapearla con su permiso. Muchos sitios web tienen en pie este tipo de restricciones por lo que es importante investigar antes de tiempo cuales sitios soportan el scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!--\\n        To discuss automated access to Amazon data please contact api-services-support@amazon.com.\\n        For information about migrating to our APIs refer to our Marketplace APIs at https://developer.amazonservices.com/ref=rm_5_sv, or our Product Advertising API at https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html/ref=rm_5_ac for advertising use cases.\\n-->\\n<!doctype html>\\n<html>\\n<head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\\n  <title>Sorry! Something went wrong!</title>\\n  <style>\\n  html, body {\\n    padding: 0;\\n    margin: 0\\n  }\\n\\n  img {\\n    border: 0\\n  }\\n\\n  #a {\\n    background: #232f3e;\\n    padding: 11px 11px 11px 192px\\n  }\\n\\n  #b {\\n    position: absolute;\\n    left: 22px;\\n    top: 12px\\n  }\\n\\n  #c {\\n    position: relative;\\n    max-width: 800px;\\n    padding: 0 40px 0 0\\n  }\\n\\n  #e, #f {\\n    height: 35px;\\n    border: 0;\\n    font-size: 1em\\n  }\\n\\n  #e {\\n    width: 100%;\\n    margin: 0;\\n    padding: 0 10px;\\n    border-radius: 4px 0 0 4px\\n  }\\n\\n  #f {\\n    cursor: pointer;\\n    background: #febd69;\\n    font-weight: bold;\\n    border-radius: 0 4px 4px 0;\\n    -webkit-appearance: none;\\n    position: absolute;\\n    top: 0;\\n    right: 0;\\n    padding: 0 12px\\n  }\\n\\n  @media (max-width: 500px) {\\n    #a {\\n      padding: 55px 10px 10px\\n    }\\n\\n    #b {\\n      left: 6px\\n    }\\n  }\\n\\n  #g {\\n    text-align: center;\\n    margin: 30px 0\\n  }\\n\\n  #g img {\\n    max-width: 90%\\n  }\\n\\n  #d {\\n    display: none\\n  }\\n\\n  #d[src] {\\n    display: inline\\n  }\\n  </style>\\n</head>\\n<body>\\n    <a href=\"/ref=cs_503_logo\"><img id=\"b\" src=\"https://images-na.ssl-images-amazon.com/images/G/01/error/logo._TTD_.png\" alt=\"Amazon.com\"></a>\\n    <form id=\"a\" accept-charset=\"utf-8\" action=\"/s\" method=\"GET\" role=\"search\">\\n        <div id=\"c\">\\n            <input id=\"e\" name=\"field-keywords\" placeholder=\"Search\">\\n            <input name=\"ref\" type=\"hidden\" value=\"cs_503_search\">\\n            <input id=\"f\" type=\"submit\" value=\"Go\">\\n        </div>\\n    </form>\\n<div id=\"g\">\\n  <div><a href=\"/ref=cs_503_link\"><img src=\"https://images-na.ssl-images-amazon.com/images/G/01/error/500_503.png\"\\n                                        alt=\"Sorry! Something went wrong on our end. Please go back and try again or go to Amazon\\'s home page.\"></a>\\n  </div>\\n  <a href=\"/dogsofamazon/ref=cs_503_d\" target=\"_blank\" rel=\"noopener noreferrer\"><img id=\"d\" alt=\"Dogs of Amazon\"></a>\\n  <script>document.getElementById(\"d\").src = \"https://images-na.ssl-images-amazon.com/images/G/01/error/\" + (Math.floor(Math.random() * 43) + 1) + \"._TTD_.jpg\";</script>\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es publicar información; ya sea crearla o actualizarla. Esto se hace por medio del método post (sigue siendo un request); con esta se manda una petición a un sitio web de práctica para agregar información al segmento de form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\\n  \"args\": {}, \\n  \"data\": \"\", \\n  \"files\": {}, \\n  \"form\": {\\n    \"Nombre\": \"Yo\"\\n  }, \\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"Content-Length\": \"9\", \\n    \"Content-Type\": \"application/x-www-form-urlencoded\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"python-requests/2.22.0\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5ec47745-ce9a6e2987ab32498b2cadb1\"\\n  }, \\n  \"json\": null, \\n  \"origin\": \"189.207.43.23\", \\n  \"url\": \"http://httpbin.org/post\"\\n}\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.post('http://httpbin.org/post', \n",
    "                  data = {'Nombre':'Yo'})\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresando al método get; nosotros podemos utilizar ciertas funciones para manipular de forma específica a donde queremos entrar de la página. En ciertos casos esto se usa para accedar a funciones específicas dentro de una API; por lo tanto, se establece una llave y un valor el cual se incluye dentro de la dirección web de la siguiente forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://httpbin.org/get?key1=value1&key2=value2&key2=value3\n"
     ]
    }
   ],
   "source": [
    "payload = {'key1': 'value1', \n",
    "           'key2': ['value2', 'value3']}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos mandar solicitudes para borrar información o solamente probar la respuesta del servidor; estos son los métodos delete y head. Para scrapping no los usaremos mucho ya que en si no queremos modificar la información del documento sino queremos extraerla. Aunque en caso de querer hacerlo debemos de contar con los permisos correspondientes ya que la misma página tiene que poner restricciones para evitar que cualquier persona modifique la página a voluntad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup4\n",
    "Ahora; los nombres de los paquetes de Python muchas veces se basan en obras de la cultura popular. Esto surge de Alicia en el país de las maravillas debido a las personas que hacen el programa se reservan los derechos de nombrarlo. Ahora bien; esta libreria es muy similar a la de requests ya que podemos hacer los requests tradicionales; pero la diferencia está en que con este paquete ustedes pueden navegar la estructura de la pagina web con mucha mayor facilidad que utilizando Python vanilla. Esto nos permite extraer la información de la página web y utilizarla para nuestras propias cuestiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASCII_SPACES',\n",
       " 'DEFAULT_BUILDER_FEATURES',\n",
       " 'NO_PARSER_SPECIFIED_WARNING',\n",
       " 'ROOT_TAG_NAME',\n",
       " '__bool__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://www.milenio.com'\n",
    "URL = BeautifulSoup(requests.get(URL).text, \"lxml\")\n",
    "dir(URL)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este objeto no es un código html, es lo que se le conoce como una sopa (tiene muchos ingredientes pero los tenemos bien catalogados). Con estos comandos podemos reobtener parámetros interesantes como con el método de requests pero en este caso podemos explorar muchas cosas más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>MILENIO - Noticias de hoy en México y el mundo - Grupo Milenio</title> title MILENIO - Noticias de hoy en México y el mundo - Grupo Milenio\n"
     ]
    }
   ],
   "source": [
    "print(URL.title, URL.title.name, URL.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de find_all nos permite buscar el tag del renglón incluyendo el texto y las demás propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/cdmx\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">CDMX</span>\n",
       " <meta content=\"1\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/monterrey\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Monterrey</span>\n",
       " <meta content=\"2\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/jalisco\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Jalisco</span>\n",
       " <meta content=\"3\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/estado-de-mexico\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Estado de México</span>\n",
       " <meta content=\"4\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/laguna\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Laguna</span>\n",
       " <meta content=\"5\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/tamaulipas\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Tamaulipas</span>\n",
       " <meta content=\"6\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/leon\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">León</span>\n",
       " <meta content=\"7\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/puebla\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Puebla</span>\n",
       " <meta content=\"8\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/hidalgo\" itemprop=\"item\">\n",
       " <span itemprop=\"name\">Hidalgo</span>\n",
       " <meta content=\"9\" itemprop=\"position\"/>\n",
       " </a>,\n",
       " <a href=\"/impreso\">Impreso</a>,\n",
       " <a href=\"/mileniotv\">Televisión</a>,\n",
       " <a href=\"/\"><img alt=\"logo-milenio\" class=\"main-logo\" src=\"/bundles/camusassets/images/logo-milenio-v2.png\"/></a>,\n",
       " <a href=\"/opinion\">Opinión</a>,\n",
       " <a href=\"/ultima-hora\">\n",
       " <span>Última Hora</span>\n",
       " </a>,\n",
       " <a href=\"/coronavirus-mexico-mundo\">\n",
       " <span>Coronavirus</span>\n",
       " </a>,\n",
       " <a href=\"/politica\">\n",
       " <span>Política</span>\n",
       " </a>,\n",
       " <a href=\"/estados\">\n",
       " <span>Estados</span>\n",
       " </a>,\n",
       " <a href=\"/policia\">\n",
       " <span>Policía</span>\n",
       " </a>,\n",
       " <a href=\"/negocios\">\n",
       " <span>Negocios</span>\n",
       " </a>,\n",
       " <a href=\"/internacional\">\n",
       " <span>Mundo</span>\n",
       " </a>,\n",
       " <a href=\"/estilo\">\n",
       " <span>Estilo</span>\n",
       " </a>,\n",
       " <a href=\"/cultura\">\n",
       " <span>Cultura</span>\n",
       " </a>,\n",
       " <a href=\"/espectaculos\">\n",
       " <span>Hey</span>\n",
       " </a>,\n",
       " <a href=\"/deportes\">\n",
       " <span>La Afición</span>\n",
       " </a>,\n",
       " <a href=\"/milenio-foros\">\n",
       " <span>Foros</span>\n",
       " </a>,\n",
       " <a href=\"/mileniovr\">\n",
       " <span>VR360</span>\n",
       " </a>,\n",
       " <a href=\"/virales\">\n",
       " <span>Virales</span>\n",
       " </a>,\n",
       " <a href=\"/content\">\n",
       " <span>InBrand</span>\n",
       " </a>,\n",
       " <a href=\"/especiales\">\n",
       " <span>Especiales</span>\n",
       " </a>,\n",
       " <a href=\"/fotogalerias\">\n",
       " <span>Fotogalerías</span>\n",
       " </a>,\n",
       " <a href=\"/cultura/laberinto\">\n",
       " <span>Laberinto</span>\n",
       " </a>,\n",
       " <a href=\"/temas/amlo\">AMLO</a>,\n",
       " <a href=\"/temas/coronavirus\">Coronavirus</a>,\n",
       " <a href=\"/temas/precio-petroleo\">Precio petróleo</a>,\n",
       " <a href=\"/temas/oscar-chavez\">Óscar Chávez</a>,\n",
       " <a href=\"/temas/hugo-lopez-gatell\">Hugo López-Gatell</a>,\n",
       " <a href=\"/temas/coronavirus-en-mexico\">Coronavirus en México</a>,\n",
       " <a href=\"/temas/milenio-a-fondo\">MILENIO A FONDO</a>,\n",
       " <a href=\"/opinion\">\n",
       " <li class=\"father\">Opinión Nacional\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/opinion\">Nacional</a>,\n",
       " <a href=\"/opinion/monterrey\">Monterrey</a>,\n",
       " <a href=\"/opinion/jalisco\">Jalisco</a>,\n",
       " <a href=\"/opinion/tamaulipas\">Tamaulipas</a>,\n",
       " <a href=\"/opinion/laguna\">Laguna</a>,\n",
       " <a href=\"/opinion/edomex\">Estado de México</a>,\n",
       " <a href=\"/opinion/puebla\">Puebla</a>,\n",
       " <a href=\"/opinion/leon\">León</a>,\n",
       " <a href=\"/opinion/hidalgo\">Hidalgo</a>,\n",
       " <a href=\"/opinion/moneros\">Moneros</a>,\n",
       " <a href=\"/ultima-hora\">\n",
       " <li class=\"father\">Última Hora\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/impreso\">\n",
       " <li class=\"father\">Impreso\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/coronavirus-mexico-mundo\">\n",
       " <li class=\"father\">Coronavirus\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/coronavirus-mexico-mundo/historias\">Coronavirus: Historias y testimonios</a>,\n",
       " <a href=\"/politica\">\n",
       " <li class=\"father\">Política\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/politica/comunidad\">Comunidad</a>,\n",
       " <a href=\"/politica/congreso\">Congreso</a>,\n",
       " <a href=\"/politica/gobierno\">Gobierno</a>,\n",
       " <a href=\"/politica/organismos\">Organismos</a>,\n",
       " <a href=\"/politica/partidos\">Partidos</a>,\n",
       " <a href=\"\">\n",
       " <li class=\"father\">Ediciones\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/cdmx\">CDMX</a>,\n",
       " <a href=\"/monterrey\">Monterrey</a>,\n",
       " <a href=\"/jalisco\">Jalisco</a>,\n",
       " <a href=\"/estado-de-mexico\">Estado de México</a>,\n",
       " <a href=\"/laguna\">Laguna</a>,\n",
       " <a href=\"/tamaulipas\">Tamaulipas</a>,\n",
       " <a href=\"/leon\">León</a>,\n",
       " <a href=\"/puebla\">Puebla</a>,\n",
       " <a href=\"/hidalgo\">Hidalgo</a>,\n",
       " <a href=\"/estados\">\n",
       " <li class=\"father\">Estados\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/estados/mas-estados\">+ Estados</a>,\n",
       " <a href=\"/policia\">\n",
       " <li class=\"father\">Policía\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/policia/violencia-genero\">Violencia de Género</a>,\n",
       " <a href=\"/policia/mas-policia\">+ Policía</a>,\n",
       " <a href=\"/negocios\">\n",
       " <li class=\"father\">Negocios\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/negocios/emprendedores\">Emprendedores</a>,\n",
       " <a href=\"/negocios/financial-times\">Financial Times</a>,\n",
       " <a href=\"/negocios/finanzas-personales\">Finanzas Personales</a>,\n",
       " <a href=\"/negocios/godinez-millennials\">Godínez Millennials</a>,\n",
       " <a href=\"/negocios/mas-negocios\">+ Negocios</a>,\n",
       " <a href=\"/internacional\">\n",
       " <li class=\"father\">Mundo\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/internacional/estados-unidos\">Estados Unidos</a>,\n",
       " <a href=\"/internacional/latinoamerica\">Latinoamérica</a>,\n",
       " <a href=\"/internacional/europa\">Europa</a>,\n",
       " <a href=\"/internacional/medio-oriente\">Medio Oriente</a>,\n",
       " <a href=\"/internacional/asia-oceania\">Asia y Oceanía</a>,\n",
       " <a href=\"/internacional/mas-internacional\">+ Mundo</a>,\n",
       " <a href=\"/estilo\">\n",
       " <li class=\"father\">Estilo\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/estilo/gastronomia\">Gastronomía</a>,\n",
       " <a href=\"/estilo/viajes\">Viajes</a>,\n",
       " <a href=\"/estilo/tu-mascota\">Tu Mascota</a>,\n",
       " <a href=\"/estilo/mas-estilo\">+ Estilo</a>,\n",
       " <a href=\"/cultura\">\n",
       " <li class=\"father\">Cultura\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/cultura/milenio-arte\">Milenio Arte</a>,\n",
       " <a href=\"/cultura/laberinto\">Laberinto</a>,\n",
       " <a href=\"/cultura/mas-cultura\">+ Cultura</a>,\n",
       " <a href=\"/espectaculos\">\n",
       " <li class=\"father\">Hey\n",
       " \t\t\t\t\t\t\t</li>\n",
       " </a>,\n",
       " <a href=\"/espectaculos/television\">Televisión</a>,\n",
       " <a href=\"/espectaculos/cine\">Cine</a>,\n",
       " <a href=\"/espectaculos/famosos\">Famosos</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL.find_all('a')[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo se pueden extraer los subenlaces que llevan a diferentes regiónes de la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/cdmx', '/monterrey', '/jalisco', '/estado-de-mexico', '/laguna', '/tamaulipas', '/leon', '/puebla', '/hidalgo', '/impreso']\n"
     ]
    }
   ],
   "source": [
    "ref = [a['href'] for a in URL.find_all('a', href=True)]\n",
    "print(ref[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el renglón cuenta con una etiqueta y una clase, esta se puede especificar dentro del método junto con atributos como href. En el siguiente ejemplo accedemos a la etiqueta li y la clase father para extraer el primer elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"father\">Opinión Nacional\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Última Hora\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Impreso\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Coronavirus\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Política\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Ediciones\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Estados\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Policía\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Negocios\n",
      "\t\t\t\t\t\t\t</li>, <li class=\"father\">Mundo\n",
      "\t\t\t\t\t\t\t</li>]\n"
     ]
    }
   ],
   "source": [
    "lista = URL.find_all('li', \n",
    "                     class_='father')\n",
    "print(lista[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta lista podemos extraer el texto con el método get_text(). Usamos el método .strip() para quitar los espacios y tabs adicionales que puede tener el formato del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos los resultados\n",
    "Categorias=[str(lista[i].get_text()).strip() \n",
    "            for i in range(0,len(lista)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Opinión Nacional', 'Última Hora', 'Impreso', 'Coronavirus', 'Política', 'Ediciones', 'Estados', 'Policía', 'Negocios', 'Mundo', 'Estilo', 'Cultura', 'Hey', 'La Afición', 'Foros', 'Sustentable', 'Autos', 'Bienes Raíces', 'VR360', 'Virales', 'InBrand', 'Especiales', 'Fotogalerías', 'Laberinto', 'Ocio', 'Aula', 'Tecnología', 'EN VIVO', 'Suscripciones', 'MILENIO CUPONES', 'Opinión Nacional', 'Última Hora', 'Impreso', 'Coronavirus', 'Política', 'Ediciones', 'Estados', 'Policía', 'Negocios', 'Mundo', 'Estilo', 'Cultura', 'Hey', 'La Afición', 'Foros', 'Sustentable', 'Autos', 'Bienes Raíces', 'VR360', 'Virales', 'InBrand', 'Especiales', 'Fotogalerías', 'Laberinto', 'Ocio', 'Aula', 'Tecnología', 'EN VIVO', 'Suscripciones']\n"
     ]
    }
   ],
   "source": [
    "print(Categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo; esto no nos quita las restricciones que nos ponen las página de internet por lo cual si intentamos importar walmart puede que el objeto cuente con diferentes categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASCII_SPACES',\n",
       " 'DEFAULT_BUILDER_FEATURES',\n",
       " 'NO_PARSER_SPECIFIED_WARNING',\n",
       " 'ROOT_TAG_NAME',\n",
       " '__bool__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL1 = 'https://www.walmart.com.mx/computadoras/laptops'\n",
    "URL1 = BeautifulSoup(requests.get(URL1).text, \"lxml\")\n",
    "dir(URL1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero al usar el método index... vemos que nuestro servidor no puede acceder a la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tag.index of <html><head>\n",
       "<title>Access Denied</title>\n",
       "</head><body>\n",
       "<h1>Access Denied</h1>\n",
       " \n",
       "You don't have permission to access \"http://www.walmart.com.mx/computadoras/laptops\" on this server.<p>\n",
       "Reference #18.1503e8ac.1589933976.a2de58dd\n",
       "</p></body>\n",
       "</html>\n",
       ">"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL1.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio!\n",
    "Entra a un departamento de tu preferencia en la página de Soriana (Repostería, Celulares, etc). Obten los siguientes datos de un producto.\n",
    "\n",
    "-  Nombre del Producto\n",
    "-  Precio actual\n",
    "\n",
    "Una vez que lo tengas, encuentra una forma de extraer esos datos para los primeros 3 productos usando un ciclo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soriana_url = 'https://www.soriana.com/soriana/es/c/Electronica/Pantallas/Pantallas/c/111'\n",
    "soriana_data = BeautifulSoup(requests.get(soriana_url).text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pantalla Samsung 65 plg 4K UHD LED Smart TV $17,490.00\n",
      "Pantalla Samsung 82 Pulg 4k UHD LED Smart TV $49,990.00\n",
      "Pantalla Ghia 43 plg HD LED $5,499.00\n",
      "Pantalla Hisense 55 plg 4K UHD LED Smart TV $9,990.00\n",
      "Pantalla Samsung 65 plg 4K UHD LED Smart TV $19,990.00\n",
      "Pantalla GHIA 50 plg UHD LED Smart TV $7,590.00\n",
      "Pantalla GHIA 39 plg HD LED $4,490.00\n",
      "Pantalla LG 43 Pulg Full HD LED Smart TV $7,390.00\n",
      "Pantalla LG 60 plg  4K UHD LED Smart TV $13,690.00\n",
      "Pantalla LG 43 Pulg 4k UHD LED Smart TV $7,990.00\n",
      "Pantalla Samsung 70 Pulg 4k UHD LED Smart TV $21,490.00\n",
      "Pantalla Westinghouse 50 plg FHD LED Smart TV $6,299.00\n",
      "Pantalla TCL 40 Pulg FHD LED Smart TV $5,799.00\n",
      "Pantalla TCL 50 Pulg 4k UHD LED Roku TV $7,990.00\n",
      "Pantalla TCL 55 Pulg 4k UHD LED Roku TV $8,999.00\n",
      "Pantalla Samsung 50 Pulg 4K UHD LED Smart TV $13,999.00\n",
      "Pantalla LG 70 plg  4K UHD LED Smart TV $20,990.00\n",
      "Pantalla Samsung 49 plg FHD LED Smart TV $9,999.00\n",
      "Pantalla Vios 39 plg FHD LED Smart TV $7,299.00\n",
      "Pantalla Samsung 43 Pulg 4K UHD LED Smart TV $10,790.00\n"
     ]
    }
   ],
   "source": [
    "for product in soriana_data.find_all('div', class_ = 'product-item'):\n",
    "    name = product.find('a', class_ = 'thumb')['title']\n",
    "    price = product.find('span', class_ = 'price').get_text()\n",
    "    print(name, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
